{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation of OpenAI's fine-tuned models\n",
    "\n",
    "This notebook evaluates the fine-tuned models from OpenAI. The difference to the `evaluation_classification.ipynb` notebook is that response is in another JSON format that contains the research object as a list of strings and taxonomy explanations. \n",
    "\n",
    "The evaluation is done by comparing the research objects of the ground truth and the predicted response."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "data_dir = os.path.join('..', '..', 'data')\n",
    "\n",
    "input_dir = os.path.join(data_dir, 'openai_fine_tune')\n",
    "\n",
    "model_name = 'nostalgic_montalcini'\n",
    "training_file = os.path.join(input_dir, f'{model_name}_train.jsonl')\n",
    "validation_file = os.path.join(input_dir, f'{model_name}_test.jsonl')\n",
    "\n",
    "evaluation_dir = os.path.join('..', '..', 'reports', 'openai_fine_tune')\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "predictions_file = os.path.join(evaluation_dir, f'{model_name}_predictions.csv')\n",
    "evaluation_file = os.path.join(evaluation_dir, f'{model_name}_evaluation.csv')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Trained fine-tuned models to evaluate\n",
    "models = [\n",
    "    'ada:ft-personal:nostalgic-montalcini-2023-07-31-09-24-03',\n",
    "    'curie:ft-personal:nostalgic-montalcini-2023-07-31-12-04-54',\n",
    "]\n",
    "\n",
    "# Temperature values to evaluate\n",
    "temperatures = [0.0, 0.25, 0.5, 0.75, 1.0]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "train = pd.read_json(training_file, lines=True)\n",
    "test = pd.read_json(validation_file, lines=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "\n",
    "def get_completion(ft_model: str, prompt: str, temperature: float = 0.0):\n",
    "    # Check if prompt ends with \"\\n\\n###\\n\\n\"\n",
    "    if prompt[-8:] != \"\\n\\n###\\n\\n\":\n",
    "        prompt += \"\\n\\n###\\n\\n\"\n",
    "\n",
    "    response = openai.Completion.create(\n",
    "        model=ft_model,\n",
    "        prompt=prompt,\n",
    "        max_tokens=1500,\n",
    "        temperature=temperature,\n",
    "        stop=[\"\\n\"],\n",
    "    )\n",
    "\n",
    "    completion = response['choices'][0]['text']\n",
    "    if completion == \"\":\n",
    "        completion = \"{}\"\n",
    "\n",
    "    data = {}\n",
    "    try:\n",
    "        data = json.loads(completion)\n",
    "    except json.decoder.JSONDecodeError:\n",
    "        print(f\"Error: {completion}\")\n",
    "\n",
    "    return data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tqdm\n",
    "\n",
    "evaluation_input = {\n",
    "    \"prompt\": [],\n",
    "    \"ground_truth\": [],\n",
    "    \"temperature\": [],\n",
    "}\n",
    "\n",
    "for model in models:\n",
    "    evaluation_input[model] = []\n",
    "\n",
    "for inx, row in tqdm.tqdm(test.iterrows(), total=len(test)):\n",
    "    test_prompt = row['prompt']\n",
    "    ground_truth = json.loads(row['completion'])\n",
    "\n",
    "    for temperature in temperatures:\n",
    "        evaluation_input['prompt'].append(test_prompt)\n",
    "        evaluation_input['ground_truth'].append(ground_truth[\"Research Object\"])\n",
    "        evaluation_input['temperature'].append(temperature)\n",
    "\n",
    "        gt_set = set(ground_truth)\n",
    "\n",
    "        for model in models:\n",
    "            model_completion = get_completion(model, test_prompt, temperature)\n",
    "            completion_set = set(model_completion.get(\"Research Object\", []))\n",
    "\n",
    "            evaluation_input[model].append(completion_set)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "evaluation_df = pd.DataFrame(evaluation_input)\n",
    "evaluation_df.to_csv(predictions_file, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for model in models:\n",
    "    for temperature in temperatures:\n",
    "        focus_df = evaluation_df[(evaluation_df['temperature'] == temperature)]\n",
    "\n",
    "        all_labels = set()\n",
    "        for labels in focus_df[\"ground_truth\"]:\n",
    "            all_labels.update(labels)\n",
    "        for labels in focus_df[model]:\n",
    "            all_labels.update(labels)\n",
    "\n",
    "        # Convert ground truth and predicted labels to binary arrays\n",
    "        mlb = MultiLabelBinarizer(classes=list(all_labels))\n",
    "        ground_truth_binary = mlb.fit_transform(focus_df[\"ground_truth\"])\n",
    "        predicted_binary = mlb.transform(focus_df[model])\n",
    "\n",
    "        # Calculate accuracy score\n",
    "        accuracy = accuracy_score(ground_truth_binary, predicted_binary)\n",
    "        precision = precision_score(ground_truth_binary, predicted_binary, average='micro')\n",
    "        recall = recall_score(ground_truth_binary, predicted_binary, average='micro')\n",
    "        f1 = f1_score(ground_truth_binary, predicted_binary, average='micro')\n",
    "\n",
    "        results = {\n",
    "            \"model\": model,\n",
    "            \"temperature\": temperature,\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "        }\n",
    "\n",
    "        evaluation_results.append(results)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Create dataframe from evaluation results\n",
    "evaluation_results_df = pd.DataFrame.from_records(evaluation_results)\n",
    "evaluation_results_df.to_csv(evaluation_file, index=False)\n",
    "evaluation_results_df"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
