{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Use ChatGPT for classification\n",
    "\n",
    "This notebook uses [OpenAI's Chat API](https://beta.openai.com/docs/api-reference/chat) to classify papers into research objects."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = os.path.join('..', '..')\n",
    "data_dir = os.path.join(base_dir, 'data', 'software_architecture')\n",
    "\n",
    "bibtext_file = os.path.join(data_dir, 'bib-text.csv')\n",
    "taxonomy_explanation_file = os.path.join(data_dir, 'taxonomy_explanation.json')\n",
    "\n",
    "reports = os.path.join(base_dir, 'reports', 'openai_chatgpt')\n",
    "os.makedirs(reports, exist_ok=True)\n",
    "\n",
    "results_file = os.path.join(reports, 'chatgpt_classification.csv')\n",
    "evaluation_file = os.path.join(reports, 'chatgpt_evaluation.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:53:58.624246Z",
     "start_time": "2023-09-17T10:53:58.611462Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(bibtext_file)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:53:58.626495Z",
     "start_time": "2023-09-17T10:53:58.614611Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "from typing import List\n",
    "from treelib import Tree\n",
    "from src.taxonomy.utils import parse_taxonomy\n",
    "\n",
    "\n",
    "def get_research_objects(tree: Tree) -> List[str]:\n",
    "    research_obj = tree.children(\"Research Object\")\n",
    "\n",
    "    objs = []\n",
    "    for obj in research_obj:\n",
    "        objs.append(obj.tag)\n",
    "\n",
    "    return objs"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:53:58.638478Z",
     "start_time": "2023-09-17T10:53:58.626371Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Build taxonomy explanation for prompt design\n",
    "\n",
    "This section creates taxonomy explanations for [zero-shot training](https://www.promptingguide.ai/techniques/zeroshot)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from src.utils.utils_json import read_json\n",
    "\n",
    "taxonomy_explanation = read_json(taxonomy_explanation_file)\n",
    "explanation_prompt = \"\"\n",
    "\n",
    "research_obj_explanation = taxonomy_explanation[\"Research Object\"]\n",
    "for key in research_obj_explanation:\n",
    "    explanation_prompt += f\"- {key}: {research_obj_explanation[key]}\\n\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:53:58.641853Z",
     "start_time": "2023-09-17T10:53:58.635644Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create prompts for openai fine-tuning\n",
    "\n",
    "* **Prompts** are created by concatenating the title and abstract of a paper.\n",
    "* **Completions** are JSON objects that contain the research fields and explanations."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "import openai\n",
    "import json\n",
    "\n",
    "\n",
    "def classify_paper(title: str, abstract: str, temperature: float) -> list[str]:\n",
    "    paper = f\"{title}\\n\\n{abstract}\"\n",
    "\n",
    "    # OpenAI chat API\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-3.5-turbo-16k\",\n",
    "        temperature=temperature,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Paper: {paper}\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"Classify the paper. Only respond with a JSON array that contains a list of research objects. The list should contain only strings. The research objects can be following: \\n\\n\" + explanation_prompt + \"\\n\\n If the paper doesn't match any respond with an empty array.\"\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    return json.loads(response.choices[0].message.content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T10:53:58.721783Z",
     "start_time": "2023-09-17T10:53:58.646841Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "temperatures = [0.0, 0.25, 0.5, 0.75, 1.0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:11:18.331281Z",
     "start_time": "2023-09-17T11:11:18.325297Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 153/153 [10:03<00:00,  3.95s/it]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "openai.api_key = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "results = []\n",
    "\n",
    "for inx, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    for temperature in temperatures:\n",
    "        taxonomy = parse_taxonomy(row['classes'])\n",
    "        gt_classification = get_research_objects(taxonomy)\n",
    "\n",
    "        title, abstract = row[\"title\"], row[\"abstract\"]\n",
    "        prediction = classify_paper(title, abstract, temperature)\n",
    "        results.append({\n",
    "            \"doi\": row[\"doi\"],\n",
    "            \"title\": title,\n",
    "            \"abstract\": abstract,\n",
    "            \"research_objects\": gt_classification,\n",
    "            \"temperature\": temperature,\n",
    "            \"predictions\": prediction\n",
    "        })"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:04:02.767856Z",
     "start_time": "2023-09-17T10:53:58.722985Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# Create a dataframe that stores the prediction results\n",
    "results_df = pd.DataFrame(results)\n",
    "\n",
    "# Write prediction results\n",
    "results_df.to_csv(results_file, index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:04:02.796318Z",
     "start_time": "2023-09-17T11:04:02.764738Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate prediction results"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\n",
    "\n",
    "all_classes = set()\n",
    "for labels in results_df['research_objects']:\n",
    "    all_classes.update(labels)\n",
    "for labels in results_df['predictions']:\n",
    "    all_classes.update(labels)\n",
    "\n",
    "# Convert ground truth and predicted labels to binary arrays\n",
    "mlb = MultiLabelBinarizer(classes=list(all_classes))\n",
    "\n",
    "evaluation_results = []\n",
    "\n",
    "for temperature in temperatures:\n",
    "    temp_df = results_df[results_df['temperature'] == temperature]\n",
    "    ground_truth_binary = mlb.fit_transform(temp_df['research_objects'])\n",
    "    predicted_binary = mlb.transform(temp_df['predictions'])\n",
    "\n",
    "    # Calculate true positives, true negatives, false positives, false negatives\n",
    "    true_positives = (predicted_binary * ground_truth_binary).sum(axis=0)\n",
    "    true_negatives = ((1 - predicted_binary) * (1 - ground_truth_binary)).sum(axis=0)\n",
    "    false_positives = (predicted_binary * (1 - ground_truth_binary)).sum(axis=0)\n",
    "    false_negatives = ((1 - predicted_binary) * ground_truth_binary).sum(axis=0)\n",
    "\n",
    "    # Calculate accuracy score\n",
    "    accuracy = accuracy_score(ground_truth_binary, predicted_binary)\n",
    "    precision = precision_score(ground_truth_binary, predicted_binary, average='micro')\n",
    "    recall = recall_score(ground_truth_binary, predicted_binary, average='micro')\n",
    "    f1 = f1_score(ground_truth_binary, predicted_binary, average='micro')\n",
    "\n",
    "    evaluation_results.append({\n",
    "        \"temperature\": temperature,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1\n",
    "    })"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:12:30.495988Z",
     "start_time": "2023-09-17T11:12:30.456562Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "data": {
      "text/plain": "   temperature  accuracy  precision    recall        f1\n0         0.00  0.222222   0.272000  0.596491  0.373626\n1         0.25  0.209150   0.268930  0.602339  0.371841\n2         0.50  0.222222   0.269341  0.549708  0.361538\n3         0.75  0.209150   0.281646  0.520468  0.365503\n4         1.00  0.202614   0.255072  0.514620  0.341085",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>temperature</th>\n      <th>accuracy</th>\n      <th>precision</th>\n      <th>recall</th>\n      <th>f1</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.00</td>\n      <td>0.222222</td>\n      <td>0.272000</td>\n      <td>0.596491</td>\n      <td>0.373626</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.25</td>\n      <td>0.209150</td>\n      <td>0.268930</td>\n      <td>0.602339</td>\n      <td>0.371841</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.50</td>\n      <td>0.222222</td>\n      <td>0.269341</td>\n      <td>0.549708</td>\n      <td>0.361538</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.75</td>\n      <td>0.209150</td>\n      <td>0.281646</td>\n      <td>0.520468</td>\n      <td>0.365503</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1.00</td>\n      <td>0.202614</td>\n      <td>0.255072</td>\n      <td>0.514620</td>\n      <td>0.341085</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Write evaluation results\n",
    "evaluation_df = pd.DataFrame(evaluation_results)\n",
    "evaluation_df.to_csv(evaluation_file, index=False)\n",
    "evaluation_df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T11:12:31.693642Z",
     "start_time": "2023-09-17T11:12:31.650560Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
