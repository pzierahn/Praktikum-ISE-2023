{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Train Multi-Label Research Field Classifier\n",
    "\n",
    "This notebook trains a multi-label classifier to predict the research fields of a paper. The classifier is trained on the Open Research Knowledge Graph (ORKG) dataset. The classifier is trained on the title and abstract of the papers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "base_dir = os.path.join('..', '..')\n",
    "data_dir = os.path.join(base_dir, 'data')\n",
    "orkg_file = os.path.join(data_dir, 'orkg', 'orkg_data.csv')\n",
    "\n",
    "#\n",
    "# Model\n",
    "#\n",
    "\n",
    "model_dir = os.path.join(base_dir, 'models', 'scincl_multi_label_classifier')\n",
    "os.makedirs(model_dir, exist_ok=True)\n",
    "\n",
    "#\n",
    "# Data\n",
    "#\n",
    "\n",
    "train_file = os.path.join(data_dir, 'scincl_classifier', 'multi_label_test.csv')\n",
    "test_file = os.path.join(data_dir, 'scincl_classifier', 'multi_label_train.csv')\n",
    "os.makedirs(os.path.dirname(train_file), exist_ok=True)\n",
    "\n",
    "#\n",
    "# Reports\n",
    "#\n",
    "\n",
    "evaluation_dir = os.path.join(base_dir, 'reports', 'scincl_classifier')\n",
    "os.makedirs(evaluation_dir, exist_ok=True)\n",
    "\n",
    "predictions_file = os.path.join(evaluation_dir, f'multi_label_predictions.csv')\n",
    "evaluation_file = os.path.join(evaluation_dir, f'multi_label_evaluation.json')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-16T16:17:33.997855Z",
     "start_time": "2023-09-16T16:17:33.994145Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kJbanBAH5H3B"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "# Pretty print json data to console\n",
    "def print_json(tag: str, data: any):\n",
    "    print(tag, json.dumps(data, indent=2, sort_keys=True))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gn223-LX5H3C"
   },
   "source": [
    "### Build Taxonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create a random sample of 5 rows\n",
    "df = pd.read_csv(orkg_file)\n",
    "df[\"doi\"] = df.doi.apply(eval).apply(list)  # convert string to array\n",
    "df[\"subfields\"] = df.subfields.apply(eval).apply(list)  # convert string to array\n",
    "df = df.fillna('')\n",
    "\n",
    "# Remove rows where the title is less than 5 characters\n",
    "df = df[df['title'].str.len() > 35]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels = set()\n",
    "\n",
    "for inx, row in df.iterrows():\n",
    "    for label in row['subfields']:\n",
    "        #print(\"label:\", label)\n",
    "        labels.add(label)\n",
    "\n",
    "labels = list(labels)\n",
    "labels.sort()\n",
    "\n",
    "label_dict = {\n",
    "    label: i for i, label in enumerate(labels)\n",
    "}\n",
    "\n",
    "reverse_label_dict = {\n",
    "    v: k for k, v in label_dict.items()\n",
    "}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Build a vector of labels for each row\n",
    "label_vectors = []\n",
    "\n",
    "for inx, row in df.iterrows():\n",
    "    label_vector = np.zeros(len(labels), dtype=int)\n",
    "    for label in row['subfields']:\n",
    "        label_vector[label_dict[label]] = 1\n",
    "    label_vectors.append(label_vector)\n",
    "\n",
    "df['label_vector'] = label_vectors"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Dataset"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "c7YAwJHH5H3D",
    "outputId": "df02d0a5-661a-4f32-a9e6-303c9c907768"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('malteos/scincl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df['text'] = df['title']\n",
    "\n",
    "for inx, row in df.iterrows():\n",
    "    if row['abstract'] != '':\n",
    "        df['text'][inx] += tokenizer.sep_token + row['abstract']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Split the dataset into train and test\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42)\n",
    "df_train.to_csv(train_file, index=False)\n",
    "df_test.to_csv(test_file, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7tZPJirg5H3O"
   },
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from datasets import DatasetDict\n",
    "\n",
    "dd = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(df_train),\n",
    "    \"test\": Dataset.from_pandas(df_test),\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5X92NoiK5H3P"
   },
   "source": [
    "#### Tokenize the text in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "juqUjGOM5H3P"
   },
   "outputs": [],
   "source": [
    "def tokenize_function(examples):\n",
    "    # tokenize the text\n",
    "    tokenized_examples = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "    )\n",
    "\n",
    "    # pad the attention masks to the same length as the input sequences\n",
    "    tokenized_examples['attention_mask'] = [\n",
    "        torch.cat([\n",
    "            torch.tensor(mask),\n",
    "            torch.zeros(512 - len(mask))\n",
    "        ])\n",
    "        for mask in tokenized_examples['attention_mask']\n",
    "    ]\n",
    "\n",
    "    return tokenized_examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gW7MwaPL5H3R"
   },
   "source": [
    "#### Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FhbPrf75H3R"
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = dd.map(tokenize_function, batched=True)\n",
    "\n",
    "# remove unnecessary columns from dataset\n",
    "tokenized_datasets = tokenized_datasets.remove_columns([\n",
    "    \"id\",\n",
    "    \"title\",\n",
    "    \"abstract\",\n",
    "    \"doi\",\n",
    "    \"research field\",\n",
    "    \"subfields\",\n",
    "    \"__index_level_0__\",\n",
    "])\n",
    "# set the format of the dataset to return PyTorch instrad og lists\n",
    "tokenized_datasets.set_format(\"torch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OEKkBPNV5H3T"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# create DataLoader objects to iterate over batches of data when training\n",
    "train_dataloader = DataLoader(tokenized_datasets[\"train\"], shuffle=True, batch_size=20)\n",
    "test_dataloader = DataLoader(tokenized_datasets[\"test\"], batch_size=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4pTgWXgu5H3T",
    "outputId": "a49db9cc-7cdf-4043-ec29-6c2e308d8744"
   },
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from transformers import AutoModelForSequenceClassification, get_scheduler\n",
    "\n",
    "# get the model\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"malteos/scincl\",\n",
    "    num_labels=len(label_dict)).to(device)\n",
    "\n",
    "# define optimizer with the learning rate and the scheduler\n",
    "optimizer = AdamW(model.parameters(), lr=1e-5)\n",
    "num_epochs = 3\n",
    "num_training_steps = num_epochs * len(train_dataloader)\n",
    "lr_scheduler = get_scheduler(\n",
    "    name=\"linear\",\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=0,\n",
    "    num_training_steps=num_training_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i93rIJFz5H3U"
   },
   "source": [
    "#### Training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r19J1JAd5H3V",
    "outputId": "593b8f33-c637-44fb-a950-16930364a28b"
   },
   "outputs": [],
   "source": [
    "model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HMkxEtPK5H3V",
    "outputId": "de6e6dab-5b56-4a21-eaf4-e0b51e93fa0d"
   },
   "outputs": [],
   "source": [
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "\n",
    "progress_bar = tqdm(range(num_training_steps))\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for batch in train_dataloader:\n",
    "        batch = {k: v.to(device) for k, v in batch.items() if k != 'text'}\n",
    "\n",
    "        # Extract the label vectors from the batch\n",
    "        label_vectors = batch['label_vector'].to(device)\n",
    "        del batch['label_vector']  # Remove the label_vector from batch\n",
    "\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        logits = outputs.logits  # Adjust this to match your model's output\n",
    "\n",
    "        # Calculate loss\n",
    "        loss = criterion(logits, label_vectors.float())  # Use BCE loss\n",
    "\n",
    "        loss.backward()\n",
    "\n",
    "        optimizer.step()\n",
    "        lr_scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "model.save_pretrained(model_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tsft3e9b5H3W"
   },
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2mSjfd-R5H3W"
   },
   "outputs": [],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SeUKcQJ05H3X"
   },
   "outputs": [],
   "source": [
    "progress_bar = tqdm(range(len(test_dataloader)))\n",
    "\n",
    "predictions = []\n",
    "for batch in test_dataloader:\n",
    "    batch = {k: v.to(device) for k, v in batch.items() if k != 'text'}\n",
    "\n",
    "    # Extract the label vectors from the batch\n",
    "    label_vectors = batch['label_vector'].to(device)\n",
    "    del batch['label_vector']  # Remove the label_vector from batch\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "\n",
    "    logits = outputs.logits\n",
    "    predictions.extend(logits.cpu().numpy())\n",
    "    progress_bar.update(1)\n",
    "\n",
    "progress_bar.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replace all values below 0 with 0. Round all values to 0 or 1\n",
    "predictions = np.array(predictions)\n",
    "predictions[predictions < 0] = 0\n",
    "predictions[predictions > 0] = 1\n",
    "predictions = predictions.round().astype(int)\n",
    "\n",
    "# Add the predictions to the test dataset\n",
    "df_test['predictions_vector'] = predictions.tolist()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_true = np.array(df_test['label_vector'].tolist())\n",
    "y_pred = np.array(predictions)\n",
    "\n",
    "# Calculate tp, tn, fp and fn  for each label\n",
    "tn = np.sum(np.logical_and(y_true == 0, y_pred == 0))\n",
    "tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n",
    "fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n",
    "fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n",
    "\n",
    "# Calculate the accuracy, precision, recall and F1 score for each label\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "precision = tp / (tp + fp)\n",
    "recall = tp / (tp + fn)\n",
    "f1 = 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "evaluation_report = {\n",
    "    \"tn\": tn.item(),\n",
    "    \"tp\": tp.item(),\n",
    "    \"fp\": fp.item(),\n",
    "    \"fn\": fn.item(),\n",
    "    \"accuracy\": accuracy,\n",
    "    \"precision\": precision,\n",
    "    \"recall\": recall,\n",
    "    \"f1\": f1\n",
    "}\n",
    "\n",
    "print_json(\"Evaluation report:\", evaluation_report)\n",
    "\n",
    "with open(evaluation_file, 'w') as file:\n",
    "    json.dump(evaluation_report, file, indent=2, sort_keys=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Calculate the accuracy, precision, recall and F1 score for multi-label classification\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred, average='micro')\n",
    "recall = recall_score(y_true, y_pred, average='micro')\n",
    "f1 = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "print(\"Accuracy:  \", accuracy)\n",
    "print(\"Precision: \", precision)\n",
    "print(\"Recall:    \", recall)\n",
    "print(\"F1:        \", f1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Map the label vectors back to the labels\n",
    "df_test['predictions'] = df_test['predictions_vector'].apply(\n",
    "    lambda x: [labels[i] for i, v in enumerate(x) if v == 1])\n",
    "\n",
    "# Sort subfields alphabetically\n",
    "df_test['subfields'] = df_test['subfields'].apply(lambda x: sorted(x))\n",
    "\n",
    "# Sort the predictions alphabetically\n",
    "df_test['predictions'] = df_test['predictions'].apply(lambda x: sorted(x))\n",
    "\n",
    "df_test[[\"doi\", \"title\", \"subfields\", \"predictions\"]]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "df_test.to_csv(predictions_file, index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Show random example from the test set with the predicted labels and the true labels for comparison\n",
    "import random\n",
    "\n",
    "for inx, row in random.sample(list(df_test.iterrows()), 10):\n",
    "    print(\"Title:       \", row['title'])\n",
    "    print(\"True labels: \", row['subfields'])\n",
    "    print(\"Predictions: \", row['predictions'])\n",
    "\n",
    "    y_true = np.array(row['label_vector'])\n",
    "    y_pred = np.array(row['predictions_vector'])\n",
    "\n",
    "    print(\"True label vector:      \", y_true)\n",
    "    print(\"Predicted label vector: \", y_pred)\n",
    "\n",
    "    # Calculate tp, tn, fp and fn  for each label\n",
    "    tn = np.sum(np.logical_and(y_true == 0, y_pred == 0))\n",
    "    fp = np.sum(np.logical_and(y_true == 0, y_pred == 1))\n",
    "    fn = np.sum(np.logical_and(y_true == 1, y_pred == 0))\n",
    "    tp = np.sum(np.logical_and(y_true == 1, y_pred == 1))\n",
    "\n",
    "    print(\"TP: \", tp)\n",
    "    print(\"TN: \", tn)\n",
    "    print(\"FP: \", fp)\n",
    "    print(\"FN: \", fn)\n",
    "\n",
    "    # Calculate accuracy, precision, recall and F1 score for each label\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp)\n",
    "    recall = tp / (tp + fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "    print(\"Accuracy:  \", accuracy)\n",
    "    print(\"Precision: \", precision)\n",
    "    print(\"Recall:    \", recall)\n",
    "    print(\"F1:        \", f1)\n",
    "\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
